{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dataset = Dataset('../../small-en', '../../small-fr', num_words=40000, buckets=[10, 20, 40, 80, 200], nthreads=20)\n",
    "# dataset.save('small-dataset')\n",
    "dataset = Dataset.load('small-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_input = tf.placeholder(tf.int32, shape=[None, None])\n",
    "english_length = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "\n",
    "french_input = tf.placeholder(tf.int32, shape=[None, None])\n",
    "french_length = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "french_input_neg = tf.placeholder(tf.int32, shape=[None, None])\n",
    "french_length_neg = tf.placeholder(tf.int32, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniform_initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "\n",
    "with tf.variable_scope('english_word_embedding'):\n",
    "    enlish_word_emb = tf.get_variable(\n",
    "    name=\"word_embedding\",\n",
    "    shape=[len(dataset.vocab['en']), 512],\n",
    "    initializer=uniform_initializer)\n",
    "    \n",
    "with tf.variable_scope('french_word_embedding'):\n",
    "    french_word_emb = tf.get_variable(\n",
    "    name=\"word_embedding\",\n",
    "    shape=[len(dataset.vocab['fr']), 512],\n",
    "    initializer=uniform_initializer)\n",
    "    \n",
    "english_embedding = tf.nn.embedding_lookup(enlish_word_emb, english_input)\n",
    "french_embedding = tf.nn.embedding_lookup(french_word_emb, french_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encoder(words, lengths, embedding, size):\n",
    "    cell_fw = tf.contrib.rnn.LSTMCell(size, initializer=uniform_initializer)\n",
    "    cell_bw = tf.contrib.rnn.LSTMCell(size, initializer=uniform_initializer)\n",
    "    _outputs, (state_bw, state_fw) = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                 cell_bw, \n",
    "                                                 embedding, \n",
    "                                                 sequence_length=lengths,\n",
    "                                                 dtype=tf.float32)\n",
    "    outputs = tf.concat((state_bw.h, state_fw.h), 1)\n",
    "    return outputs\n",
    "\n",
    "with tf.variable_scope('english_encoder'):\n",
    "    english_encoder = encoder(english_input, english_length, english_embedding, 512) \n",
    "    \n",
    "with tf.variable_scope('french_encoder') as scope:\n",
    "    french_encoder = encoder(french_input, french_length, french_embedding, 512)  \n",
    "    scope.reuse_variables()\n",
    "    french_encoder_neg = encoder(french_input_neg, french_length_neg, french_embedding, 512)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('pairwise_distance'):\n",
    "    pairwise_distance = tf.reduce_sum(tf.square(english_encoder), 1, keep_dims=True) +\\\n",
    "        tf.transpose(tf.reduce_sum(tf.square(french_encoder), 1, keep_dims=True)) -\\\n",
    "        2. * tf.matmul(english_encoder, tf.transpose(french_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    with tf.name_scope('triplet_loss'):\n",
    "        triplet_loss = tf.nn.relu(tf.reduce_sum(tf.square(english_encoder - french_encoder), 1) + 0.2 -\\\n",
    "                       tf.reduce_sum(tf.square(english_encoder - french_encoder_neg), 1))\n",
    "    with tf.name_scope('distance_loss'):\n",
    "        dist_loss = tf.reduce_sum(tf.square(english_encoder - french_encoder), 1)\n",
    "    loss = tf.reduce_mean(triplet_loss + dist_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "    grads, params = zip(*optimizer.compute_gradients(loss))\n",
    "    clipped_grads, _ = tf.clip_by_global_norm(grads, 20.)\n",
    "    train_op = optimizer.apply_gradients(zip(clipped_grads, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('./logs', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 38919420) |                   | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33% (12936960 of 38919420) |####          | Elapsed Time: 0:25:12 ETA: 0:50:38"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch %d' % (epoch + 1))\n",
    "    total_loss = 0.\n",
    "    iters = 0\n",
    "    for e_x, e_length, f_x, f_length in dataset.iterate_minibatches(128):\n",
    "        \n",
    "        feed_dict = {\n",
    "            english_input: e_x,  \n",
    "            english_length: e_length,\n",
    "            french_input: f_x,\n",
    "            french_length: f_length\n",
    "        }\n",
    "\n",
    "        distances = sess.run(pairwise_distance, feed_dict=feed_dict)\n",
    "        np.fill_diagonal(distances, np.inf)\n",
    "        perm = np.argmin(distances, axis=1)\n",
    "        \n",
    "        feed_dict = {\n",
    "            english_input: e_x,  \n",
    "            english_length: e_length,\n",
    "            french_input: f_x,\n",
    "            french_length: f_length,\n",
    "            french_input_neg: f_x[perm],\n",
    "            french_length_neg: f_length[perm]\n",
    "        }\n",
    "        _, c, summary = sess.run([train_op, loss, merged], feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, iters)\n",
    "        total_loss += c\n",
    "        iters += 1\n",
    "    print('Average loss on train: %.3f' % (total_loss / iters))\n",
    "    total_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
